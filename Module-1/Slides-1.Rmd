---
title: "Financial Time Series"
author: "P. Hénaff"
date: "Version: `r format(Sys.Date(), '%d %b %Y')`"

output:
  beamer_presentation:
    colortheme: dolphin
    theme: Montpellier
  ioslides_presentation: default
  slidy_presentation: default
header-includes:
- \usepackage[utf8]{inputenc}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{xcolor}

bibliography: ../library.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r load-libraries, message=FALSE}
library(quantmod)
library(xts)
library(hornpa)
library(lubridate)
library(xtable)
library(PerformanceAnalytics)
library(TTR)
library(SuppDists)
library(roll)
library(Hmisc)
library(nFactors)
library(kableExtra)
library(latex2exp)
library(corrplot)

get.src.folder <- function() {
  path.expand("../GP/src")
}

get.data.folder <- function() {
  path.expand("../GP/data")
}

data.path <- function(filename) {
  file.path(get.data.folder(), filename)
}

graphics.path <- function(filename) {
  file.path(path.expand("../Graphics"), filename)
}

source(file.path(get.src.folder(), 'utils.R'))
source(file.path(get.src.folder(), 'FileUtils.R'))
source(file.path(get.src.folder(), 'PowerLawUtils.R'))
source(file.path(get.src.folder(), 'PlotUtils.R'))

```

## Financial Time Series (daily OHLC)

```{r echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
get.prices<-function(sym, from="2004-12-31"){
  prices<-getSymbols(sym, from, auto.assign=FALSE)
  p<-Ad(prices)
  colnames(p) <- sym
  list(OHLC=prices, AC=p)
}

price.series.file <- data.path("price.series.slides.1.rda")
symbols <- c("AMZN", "GOOG", "AAPL", "QQQ", "DIA", "SPY", "PG", "KO")

if(!file.exists(price.series.file)) {

  price_series <- NULL
  quotes <- vector("list", length(symbols))
  i <- 1
  for(s in symbols) {
    res <- get.prices(s)
    if(is.null(price_series)) {
      price_series <- res$AC
    } else {
      price_series <- merge.xts(price_series, res$AC)
    }
    quotes[[i]] <- res$OHLC
    i <- i + 1
  }
  names(quotes) <- symbols
  save(price_series, quotes, file=price.series.file)
} else {
  load(price.series.file)
}
```

```{r, fig.height=5, warning=FALSE, message=FALSE}
p <- quotes$AMZN['2004-12-31/']

chartSeries(p[100:110,], type="bars")
```
Barre à gauche = valeur d'entrée, bare à droite = valeur de sortie
barre orange --> perte


## Daily Return - AMZN

\[
r_t = \log\left(\frac{P_t}{P_{t-1}}\right)
\]

!! $P_t$ désigne ici l'adjusted close.

```{r daily-return, include=FALSE, echo=FALSE}
return_series <- ROC(price_series, n=1, na.pad=FALSE)
```

```{r plot-return, fig.height=5}
plot(return_series$AMZN, col="red", main = "Daily return")
```

## Daily Return - AMZN - COMMENTS

on peut voir beaucoup de rendements avec des valeurs assez élevées (ici c'est particulier, normalement, on a pas mal de rendements avec des valeurs assez négatives)  --> fat tails
La volatilité n'a pas l'air stationnaire (périodes de vola élevée suivies de périodes de vola faible)



## Histogram of daily return - AMZN

```{r, ret-hist, echo=FALSE, fig.height=5}
r.AMZN <- return_series$AMZN
r.AMZN <- r.AMZN[!is.na(r.AMZN)]
hist(r.AMZN, breaks=40, freq=FALSE, main='')
rug(r.AMZN)
curve(dnorm(x, mean=mean(r.AMZN), sd=sd(r.AMZN)), add=TRUE, col='red', lwd=3, xaxt='n')
```
On peut voir qu'on a pas mal de valeurs extrêmes

## Analysis of return distribution - AMZN

```{r, qqplot-normal, echo=FALSE, fig.height=5}
qqnorm(as.vector(r.AMZN),main=NULL)
qqline(as.vector(r.AMZN),col='red', lwd=2)
```

## Analysis of return distribution - COMMENTS

QQ-Plot (on calcule un certain nb de quantiles qu'on peut définir)
En rouge c'est si les sample quantiles correspondaient aux quantiles théoriques de la loi normale.
Un quantile 1% c'est la valeur tq 1% des valeurs de la distribution sont en-dessous de celle-ci.
Ici, on observe un décrochagedes quantiles inférieurs wrt ceux théoriques, ex: la valeur tq 1% des valeurs sont en-dessous est plus basse, i.e.on a plus d'extrêmes négatifs (fat tails). Même observation pour les quantiles supérieurs.

## Moments of daily returns

```{r, ret-moments, echo=FALSE}
statNames <- c("mean", "std dev", "skewness", "kurtosis")
mo <- matrix(NA, nrow=length(symbols), ncol=length(statNames))
n <- 1
for(s in symbols) {
  ts <- return_series[, s]
  mo[n,] <- c(mean(ts), sd(ts), skewness(ts), kurtosis(ts))
  n <- n+1
}

colnames(mo) <- statNames
rownames(mo) <- symbols

kable(mo, "latex", booktabs=T)
```

## Moments of daily returns - COMMENTS

On peut voir que ya pas mal de skew < 0, i.e. on a pas mal de valeurs extrêmes négatives (negative skew, i.e. mean left to median left to mode whereas everything is equal in gaussian). skew = moment centré réduit d'ordre 3.
kurtosis non-normalisé = moment centré d'ordre 4 (kurt = 3 pour un gaussienne) --> on observe des kurt bien > 3, i.e. les valeurs sont beaucoup plus concentrées autour de la moyenne

## Autocorrelation of Returns (AMZN)


```{r, label=autocor, echo=FALSE, fig.height=5}
op <- par(mfrow=c(1,2))
acf(r.AMZN, ylim=c(0, .6), main='autocorrelation of r(t)')
acf(abs(r.AMZN), ylim=c(0, .6), main='autocorrelation of |r(t)|')
par(op)
```

## Autocorrelation of Returns - COMMENTS

On peut voir que y'a pas beaucoup de corrélation entre les daily returns, mais c'est beaucoup plus significatif entre les valeurs absolues des daily returns !! En gros ca confirme notre hypothèse de persistance de la vola : ya des périodes de forte vola dpnc grosses valeurs abs de returns et des périodes de faible vola.

## Rescaling daily return by $\sigma(r_{t-1})$ [@Chen-2008]

$$
z_t = \frac{r_t}{\sigma(|r_{t-1}|)}
$$
L'idée c'est de rescale les daily returns non pas en fonction de la vola totale des returns mais de la vola calculée sur une partie des returns : les returns qui, au jour précédent, ont eu le même ordre de grandeur de returns que le return qu'on traite actuellement.


The density of $z_t$ can be approximated by a power law. See paper for details of calculation.

$$
\left.\begin{aligned}
p(z_t) & = \frac{\alpha-1}{z_{min}} \left(\frac{z_t}{z_{min}} \right)^{-\alpha} \\
Pr(z_t > x) & = \left(\frac{x}{z_{min}} \right)^{-\alpha+1}
\end{aligned}
\right\} z_t > z_{min}
$$


## Rescaling of daily return by $\sigma(|r_{t-1}|)$

```{r, powerlaw-1, echo=FALSE, warning=FALSE, message=FALSE, fig.height=6, fig.align="center"}

nb <- 8
params = list(lambda=0.94, nb.init=250)
ticker <- 'AMZN'
plot.cum.cond.r(r=r.AMZN, ticker = ticker, sgn='all',NBIN=nb, model='abs', params=params)
```

## Rescaling of daily return by $\sigma(|r_{t-1}|)$ - COMMENTS

Ici, on a séparé les abs(returns) en 8 bins, et en gros on voit qu'on a quasi les mêmes densités de probas pour chaque bin (cdf dans l'autre sens, genre P(X>x)) donc c'est un bon point. Bon par contre ca marche pas pour tous les stocks.

## Unconditional distribution of return

The Johnson family of distributions is formed by various transformations
of the normal density. Let $X$ be the observed data, and define $Z$ by:

$$Z = \gamma + \delta \ln \left( g \left( \frac{X-\xi}{\lambda} \right) \right)$$

where: $$g(u) = \left\{ \begin{array}{ll}
u & SL \\
u + \sqrt{1+u^2} & SU\\
\frac{u}{1-u} & SB \\
e^u & SN
\end{array} \right.$$

$X$ follows a Johnson distribution if $Z$ is normal.

## Fitted Johnson SU distribution - AMZN (1)

```{r, johnson-1, label=JohnsonFit, echo=FALSE}
johnson.fit <- JohnsonFit(r.AMZN)
kable(as.data.frame(johnson.fit), "latex", booktabs=TRUE) %>% kable_styling(position="center")
```


```{r, johnson-2, label=empirical-vs-fitted, echo=F}
m1 <- moments(r.AMZN)
st <- sJohnson(johnson.fit)
m2 = c(st$Mean, sqrt(st$Variance), st$Skewness, st$Kurtosis)
df.stats <- data.frame(cbind(sample=m1, johnson=m2))
kable(df.stats, "latex", booktabs=TRUE) %>% kable_styling(position="center")
```

## Fitted Johnson SU distribution - AMZN (2)

```{r, johnson-3, echo=FALSE, fig.height=6}
pp3 <- function(x) format(x, scientific=TRUE, digits=3)

t2 = substitute(paste(
  gamma, ": ", g, " ",
  delta, ": ", d, " ",
  xi, ": ", x, " ",
  lambda, ": ", l), list(g=pp3(johnson.fit$gamma), d=pp3(johnson.fit$delta),
                         x=pp3(johnson.fit$xi), l=pp3(johnson.fit$lambda)))

hist(r.AMZN,freq=FALSE,breaks=40, main=t2)
plot(function(x)dJohnson(x,johnson.fit),-.1,.1,col='red', lwd=3,add=TRUE)
```
Les daily returns ont l'air de plus suivre une Johnson SU distribution plutôt que juste une gaussian.

## Fitted Johnson SU distribution - AMZN (3)

```{r, johnson-3, label=qq-johnson, echo=FALSE, fig.height=6}
# QQ plot with johnson distribution
qqJohnson <- function(y, params, n=100, abline=TRUE, ...) {
  u <- seq(from=1/(n+1), by=1/(n+1), length=n)
  q <- qJohnson(u, params)
  if (abline) {
    ret <- qqplot(q, as.vector(y), ...)
    abline(0,1)
  }
  else {
    ret <- qqplot(q, as.vector(y), ...)
  }
  invisible(ret)
}

op <- par(mfrow=c(1,2))
qqJohnson(r.AMZN, johnson.fit, main="Johnson SU")
qqnorm(r.AMZN, main="Normal")
qqline(r.AMZN)
par(op)
```
Effectivement, le QQ Plot est beaucoup mieux.

## Correlation between assets (NASDAQ)

```{r, cor-0, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE, cache=TRUE}
folder <- 'NASDAQ'
tickers <- get.tickers(folder)[1:150]
ts.all <- get.all.ts(folder, tickers, dt.start = dmy('01Mar2007'), combine = TRUE)
```

<<<<<<< Updated upstream
```{r, cor-1, echo=FALSE, message=FALSE}
=======
```{r, cor-1, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE, cache=TRUE}
>>>>>>> Stashed changes
nb.ev = 6
nb.obs <- 252

dt.start <- dmy("01Aug2009")
idx.start <- closest.index(ts.all, dt.start)
idx <- seq(idx.start, length.out=nb.obs)

cor.NASDAQ <- cor(ts.all[idx,])

corrplot(cor.NASDAQ, type="upper", order="hclust", tl.pos='n')
```

## Correlation between assets (NASDAQ) - COMMENTS

Tous les assets ne sont pas corrélés, y'a l'air d'y avoir un clustering possible (cf exos).

## Correlation between assets

```{r, cor-2, echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}
## NASDAQ
folder <- 'NASDAQ'
tickers <- get.tickers(folder)[1:150]
ts.all <- get.all.ts(folder, tickers, dt.start = dmy('01Mar2007'), combine = TRUE)
```

```{r, cor-3, echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}
nb.ev = 6
nb.obs <- 252

dt.start <- dmy("01Aug2009")
idx.start <- closest.index(ts.all, dt.start)
idx <- seq(idx.start, length.out=nb.obs)

res.pca.1 <- prcomp(ts.all[idx,], scale=TRUE)

# normalized eigenvalues
norm.ev <- res.pca.1$sdev^2
norm.ev <- norm.ev/sum(norm.ev)

large.ev.1 <- norm.ev[1:nb.ev]
names(large.ev.1) <- paste("PC", seq_along(large.ev.1))

dt.start <- dmy("01Aug2012")
idx.start <- closest.index(ts.all, dt.start)
idx <- seq(idx.start, length.out=nb.obs)

res.pca.2 <- prcomp(ts.all[idx,], scale=TRUE)

# normalized eigenvalues
norm.ev <- res.pca.2$sdev^2
norm.ev <- norm.ev/sum(norm.ev)

large.ev.2 <- norm.ev[1:nb.ev]
names(large.ev.2) <- paste("PC", seq_along(large.ev.2))
```

```{r, cor-4, echo=FALSE, fig.height=5, cache=FALSE, warning=FALSE, message=FALSE, cache=TRUE}
plot.1 <- barplot(100*large.ev.1, ylim=c(0,60), col="blue", ylab="Contribution (%)",
                  main="First PCs of 150 NASDAQ stocks, August 2009")
lines(plot.1, 100*cumsum(large.ev.1), type='b', pch=5, col="red", lty=2)
legend("right", legend=c("Contribution ratio", "cumulative contribution"),
       col=c("blue", "red", "green"), lty=1:3, cex=0.8)
```

## Correlation between assets - COMMENTS

On peut voir que les returns des actifs du NASDAQ étaient tous majoritairment influencés par un seul facteur en 2009, ce qui est dû la crise des subprimes. Effectivmement, on était dans les retombées de celle ci. Donc là on voit que la diversification ca marche que quand on en a pas besoin mdr (ici ils ont tous subis la crise).

## Correlation between assets

```{r, cor-5, echo=FALSE, fig.height=5}
plot.2 <- barplot(100*large.ev.2, ylim=c(0,60), col="blue", ylab="Contribution (%)",
                  main="First PCs of 150 NASDAQ stocks, August 2012")
lines(plot.2, 100*cumsum(large.ev.2), type='b', pch=5, col="red", lty=2)
legend("right", legend=c("Contribution ratio", "cumulative contribution"),
       col=c("blue", "red"), lty=1:2, cex=0.8)
```
Quelques années plus tard, le 1er facteur a moins d'importance, dû à l'atténuation des effets de la crise et que les entreprises saines ont réussi à se redresser.

## How many dimensions in a market?

Significance level (95%) for eigenvalues (252 observations, 127 variables):


```{r, cor-6, fig.height=5, warning=FALSE}
dt.start <- dmy("01Aug2009")
idx.start <- closest.index(ts.all, dt.start)
idx <- seq(idx.start, length.out=nb.obs)

ev <- eigen(cor(ts.all[idx,]))
ap <- parallel(subject=length(idx), var=ncol(ts.all), rep=200, cent=0.05)

ev <- eigen(cor(ts.all[idx,])) # get eigenvalues
ap <- parallel(subject=length(idx),var=ncol(ts.all),
               rep=200,cent=.05)

y <- ev$values[1:8]
names(y) <- paste("PC", seq_along(y))

gp <- gap.plot.helper(seq_along(y), y, from=4, to=52, ylim=c(0, 60), col="blue", xlab="PC",
                      ylab="value", main="NASDAQ Eigenvalues (150 stocks) - 01 Aug 2009")
lines(seq_along(y), ap$eigen$qevpea[1:8], col="red")
legend("right", legend=c("Eigenvalue", "Significance level"), col=c("blue", "red"),
       lty=c(NA, 1), pch=c(1, NA))
minor.tick(ny=2)

```
En 2009, on observe 1 ou 2 dimensions importantes au sein du NASDAQ (la première eigenval est trop haute, on peut pas la voir)

## How many dimensions in a market?

```{r, cor-7, fig.height=5, warning=FALSE}
dt.start <- dmy("01Aug2012")
idx.start <- closest.index(ts.all, dt.start)
idx <- seq(idx.start, length.out=nb.obs)

ev <- eigen(cor(ts.all[idx,]))
ap <- parallel(subject=length(idx), var=ncol(ts.all), rep=200, cent=0.05)

ev <- eigen(cor(ts.all[idx,])) # get eigenvalues
ap <- parallel(subject=length(idx),var=ncol(ts.all),
               rep=200,cent=.05)

y <- ev$values[1:8]
names(y) <- paste("PC", seq_along(y))

gp <- gap.plot.helper(seq_along(y), y, from=5, to=30, ylim=c(0, 40), col="blue", xlab="PC",
                      ylab="Eigenvalue", main="NASDAQ Eigenvalues (150 stocks) - 01 Aug 2013")
lines(seq_along(y), ap$eigen$qevpea[1:8], col="red")
legend("right", legend=c("Eigenvalue", "Significance level"), col=c("blue", "red"),
       lty=c(NA, 1), pch=c(1, NA))
minor.tick(ny=2)
```
En 2013, on pourrait dire 2 à 4 facteurs.


## Summary

To summarize, empirical observations show that the distribution of
returns exhibit features that strongly repart from the classical
hypothesis of independence and normality. We find:

1.  no evidence of linear autocorrelation of return, however,

2.  there is an observable autocorrelation of $|r_t|$ and $r_t^2$,
suggesting autocorrelation in the volatility of return,

3.  we also observe large excess kurtosis, which is incompatible with
normal density,

4.  The rank of a broad stock market such as the NASDAQ is probably much lower than the number of stocks.


## Bibliography
